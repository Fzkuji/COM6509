{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Part 1 Brief \n",
    "\n",
    "## Deadline: Friday, December 3, 2021 at 15:00 hrs\n",
    "\n",
    "## Number of marks available for Part 1: 25\n",
    "\n",
    "## Scope: Sessions 1 to 5\n",
    "\n",
    "### Please READ the whole assignment first, before starting to work on it.\n",
    "\n",
    "### How and what to submit\n",
    "\n",
    "A. A **Jupyter Notebook** with the code in all the cells executed and outputs displayed.\n",
    "\n",
    "B. Name your Notebook **COM4509-6509_Assignment_Part1_Username_XXXXXX.ipynb** where XXXXXX is your username such as such as abc18de.  \n",
    "\n",
    "C. Upload a .zip file to Blackboard before the deadline that contains the Jupyter Notebook in B and any other files requested for the solution of Part 2 of the Assignment (Dr Lu will be in charge of releasing this part). \n",
    "\n",
    "D. **NO DATA UPLOAD**: Please do not upload the data files used in this Notebook. We have a copy already. \n",
    "\n",
    "\n",
    "### Assessment Criteria \n",
    "\n",
    "* Being able to use numpy and pandas to preprocess a dataset.\n",
    "\n",
    "* Being able to use numpy to build a machine learning pipeline for supervised learning. \n",
    "\n",
    "* Being able to follow the steps involved in an end-to-end project in machine learning.\n",
    "\n",
    "* Being able to use scikit-learn to design a machine learning model pipeline\n",
    "\n",
    "\n",
    "### Late submissions\n",
    "\n",
    "We follow Department's guidelines about late submissions, i.e., a deduction of 5% of the mark each working day the work is late after the deadline. NO late submission will be marked one week after the deadline because we will release a solution by then. Please read [this link](https://sites.google.com/sheffield.ac.uk/comughandbook/general-information/assessment/late-submission) if you are taking COM4509 or read [this link](https://sites.google.com/sheffield.ac.uk/compgtstudenthandbook/menu/assessment/late-submission) if you are taking COM6509. \n",
    "\n",
    "### Use of unfair means \n",
    "\n",
    "**\"Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.\"** (from the students Handbook). Please carefully read [this link](https://sites.google.com/sheffield.ac.uk/comughandbook/general-information/assessment/unfair-means) on what constitutes Unfair Means if not sure, for COM4509. If you are taking COM6509, please read [this link](https://sites.google.com/sheffield.ac.uk/compgtstudenthandbook/menu/referencing-unfair-means) if you are not sure what is Unfair means. If you still have questions, please ask your Personal tutor or the Lecturers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A dataset of air quality\n",
    "\n",
    "The dataset you will use in this assignment comes from a popular machine learning repository that hosts open source datasets for educational and research purposes, the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). We are going to use regularised ridge regression and random forests for predicting air quality. The description of the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Air+Quality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./AirQualityUCI.zip', <http.client.HTTPMessage at 0x1da221d6a08>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "doq = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip\"\n",
    "pat_sav = \"./AirQualityUCI.zip\"\n",
    "urllib.request.urlretrieve(doq, pat_sav)\n",
    "#urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00360/AirQualityUCI.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip = zipfile.ZipFile('./AirQualityUCI.zip', 'r')\n",
    "for name in zip.namelist():\n",
    "    zip.extract(name, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .csv version of the file has some typing issues, so we use the excel version\n",
    "import pandas as pd\n",
    "air_quality_full = pd.read_excel('./AirQualityUCI.xlsx', usecols=range(2,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the rows in the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>NMHC(GT)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7623</th>\n",
       "      <td>1.4</td>\n",
       "      <td>1442.75</td>\n",
       "      <td>-200</td>\n",
       "      <td>16.902372</td>\n",
       "      <td>1207.00</td>\n",
       "      <td>645.1</td>\n",
       "      <td>502.00</td>\n",
       "      <td>208.0</td>\n",
       "      <td>1430.75</td>\n",
       "      <td>1673.25</td>\n",
       "      <td>6.875000</td>\n",
       "      <td>76.924999</td>\n",
       "      <td>0.768237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5555</th>\n",
       "      <td>0.8</td>\n",
       "      <td>905.00</td>\n",
       "      <td>-200</td>\n",
       "      <td>5.247306</td>\n",
       "      <td>775.75</td>\n",
       "      <td>163.0</td>\n",
       "      <td>850.75</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1327.25</td>\n",
       "      <td>825.00</td>\n",
       "      <td>15.200000</td>\n",
       "      <td>77.974998</td>\n",
       "      <td>1.338227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338</th>\n",
       "      <td>2.1</td>\n",
       "      <td>1007.50</td>\n",
       "      <td>-200</td>\n",
       "      <td>10.056784</td>\n",
       "      <td>979.75</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>754.50</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1465.25</td>\n",
       "      <td>824.50</td>\n",
       "      <td>32.474999</td>\n",
       "      <td>21.175000</td>\n",
       "      <td>1.017309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>3.3</td>\n",
       "      <td>1234.25</td>\n",
       "      <td>-200</td>\n",
       "      <td>15.387979</td>\n",
       "      <td>1160.75</td>\n",
       "      <td>664.0</td>\n",
       "      <td>629.25</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1385.50</td>\n",
       "      <td>1318.00</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>61.675001</td>\n",
       "      <td>0.592622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4461</th>\n",
       "      <td>-200.0</td>\n",
       "      <td>1048.00</td>\n",
       "      <td>-200</td>\n",
       "      <td>3.322503</td>\n",
       "      <td>671.75</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1050.00</td>\n",
       "      <td>-200.0</td>\n",
       "      <td>1437.75</td>\n",
       "      <td>555.75</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>59.949999</td>\n",
       "      <td>1.916836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CO(GT)  PT08.S1(CO)  NMHC(GT)   C6H6(GT)  PT08.S2(NMHC)  NOx(GT)  \\\n",
       "7623     1.4      1442.75      -200  16.902372        1207.00    645.1   \n",
       "5555     0.8       905.00      -200   5.247306         775.75    163.0   \n",
       "4338     2.1      1007.50      -200  10.056784         979.75   -200.0   \n",
       "6038     3.3      1234.25      -200  15.387979        1160.75    664.0   \n",
       "4461  -200.0      1048.00      -200   3.322503         671.75   -200.0   \n",
       "\n",
       "      PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)          T         RH  \\\n",
       "7623        502.00    208.0       1430.75      1673.25   6.875000  76.924999   \n",
       "5555        850.75     48.0       1327.25       825.00  15.200000  77.974998   \n",
       "4338        754.50   -200.0       1465.25       824.50  32.474999  21.175000   \n",
       "6038        629.25    200.0       1385.50      1318.00   6.300000  61.675001   \n",
       "4461       1050.00   -200.0       1437.75       555.75  25.400000  59.949999   \n",
       "\n",
       "            AH  \n",
       "7623  0.768237  \n",
       "5555  1.338227  \n",
       "4338  1.017309  \n",
       "6038  0.592622  \n",
       "4461  1.916836  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_quality_full.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable corresponds to the CO(GT) variable of the first column. The following columns correspond to the variables in the feature vectors, *e.g.*, PT08.S1(CO) is $x_1$ up until AH which is $x_D$. The original dataset also has a date and a time columns that we are not going to use in this assignment.\n",
    "\n",
    "### Removing instances \n",
    "\n",
    "The dataset has missing values tagged with a -200 value. To simplify the design of the machine learning models in this assignment, we perform the following two operations to the dataset right from the beginning:\n",
    "\n",
    "* we will remove the rows for which the target variable has missing values. We are doing supervised learning so we need all our data observations to have known target values.\n",
    "\n",
    "* we will remove features with more than 20% of missing values. \n",
    "\n",
    "The code below performs both operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first remove the rows for which there are missing values in the target feature\n",
    "air_quality = air_quality_full.loc[air_quality_full.iloc[:, 0]!=-200, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now remove the columns (features) for which there are more that 20% of missing values\n",
    "import numpy as np\n",
    "ndata, ncols = np.shape(air_quality) # number of data observations and number of columns in the dataframe\n",
    "pmissing = np.empty(ncols)         # An empty vector that will keep the percentage of missing values per feature\n",
    "for i in range(ncols):\n",
    "    pmissing[i] = (air_quality.iloc[:, i]==-200).sum()/ndata # Computes the percentage of missing values per column\n",
    "air_quality = air_quality.loc[:, pmissing < 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset \n",
    "\n",
    "Before designing any machine learning model, we need to set aside the test data. We will use the remaining training data for fitting the model. *It is important to remember that the test data has to be set aside before preprocessing*. \n",
    "\n",
    "Any preprocessing that you do has to be done only on the training data and several key statistics need to be saved for the test stage.  Separating the dataset into training and test before any preprocessing has happened help us to recreate the real world scenario where we will deploy our system and for which the data will come without any preprocessing.\n",
    "\n",
    "Furthermore, we are going to use *hold-out validation* for validating our predictive model so we need to further separate the training data into a training set and a validation set.\n",
    "\n",
    "We split the dataset into a training set, a validation set and a test set. The training set will have 70% of the total observations, the validation set 15% and the test set, the remaining 15%. For making the random selections of the training and validation sets **make sure that you use a random seed that corresponds to the last five digits of your student UCard**. In the code below, I have used 55555 as an example of my random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(61116)                 # Make sure you use the last five digits of your student UCard as your seed\n",
    "index = np.random.permutation(ndata)  # We permute the indexes \n",
    "N = np.int64(np.round(0.70*ndata))    # We compute N, the number of training instances\n",
    "Nval = np.int64(np.round(0.15*ndata)) # We compute Nval, the number of validation instances   \n",
    "Ntest = ndata - N - Nval              # We compute Ntest, the number of test instances\n",
    "data_training_unproc = air_quality.iloc[index[0:N], :].copy() # Select the training data\n",
    "data_val_unproc = air_quality.iloc[index[N:N+Nval], :].copy() # Select the validation data\n",
    "data_test_unproc = air_quality.iloc[index[N+Nval:ndata], :].copy() # Select the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assigment is divided into two sections. In the **first section**, you will design a regularised ridge regression model trained with stochastic gradient descent. You will write all the code from scratch. You should not use any library that already implements any of the routines considered in this section, for example, scikit-learn. In the **second section**, you will design a random forests model and you are allowed to use scikit-learn in this section.\n",
    "\n",
    "When writing your code, you will find out that there are operations that are repeated at least twice. We will assign marks for use of Python functions and for commenting your code. The marks will be assigned as:\n",
    "\n",
    "* Did you include Python functions to solve the question and avoid repeating code? (**1 mark**)\n",
    "* Did you comment your code to make it readable to others? (**1 mark**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Using regularised ridge regression to predict air quality (10 marks)\n",
    "\n",
    "**DO NOT USE scikit-learn or any other machine learning library for the questions on this section. You are meant to write Python code from scratch. You can use Pandas and Numpy. Using scikit-learn or any other machine learning library for the questions in this section will give ZERO marks. No excuse will be accepted.**\n",
    "\n",
    "Regularisation is a technique commonly used in Machine Learning to prevent overfitting. It consists on adding terms to the objective function such that the optimisation procedure avoids solutions that just learn the training data. Popular techniques for regularisation in Supervised Learning include [Lasso Regression](https://en.wikipedia.org/wiki/Lasso_(statistics)), [Ridge Regression](https://en.wikipedia.org/wiki/Tikhonov_regularization) and the [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization). \n",
    "\n",
    "In this part of the Assignment, you will be looking at Ridge Regression and implementing equations to optimise the objective function using the update rules for stochastic gradient descent. You will use those update rules for making predictions on the Air Quality dataset.\n",
    "\n",
    "## 1.1 Ridge Regression\n",
    "\n",
    "Let us start with a data set for training $\\mathcal{D} = \\{\\mathbf{y}, \\mathbf{X}\\}$, where the vector $\\mathbf{y}=[y_1, \\cdots, y_N]^{\\top}$ and $\\mathbf{X}$ is the design matrix from Lab 4, this is, \n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{X} = \n",
    "                \\begin{bmatrix}\n",
    "                        1 & x_{1,1} & \\cdots & x_{1, D}\\\\\n",
    "                        1 & x_{2,1} & \\cdots & x_{2, D}\\\\\n",
    "                   \\vdots &  \\vdots\\\\\n",
    "                        1 & x_{N,1} & \\cdots & x_{N, D}\n",
    "                \\end{bmatrix}\n",
    "               = \n",
    "               \\begin{bmatrix}\n",
    "                      \\mathbf{x}_1^{\\top}\\\\\n",
    "                       \\mathbf{x}_2^{\\top}\\\\\n",
    "                          \\vdots\\\\\n",
    "                        \\mathbf{x}_N^{\\top}\n",
    "                \\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "Our predictive model is going to be a linear model\n",
    "\n",
    "$$ f(\\mathbf{x}_i) = \\mathbf{w}^{\\top}\\mathbf{x}_i,$$\n",
    "\n",
    "where $\\mathbf{w} = [w_0\\; w_1\\; \\cdots \\; w_D]^{\\top}$.\n",
    "\n",
    "The **objective function** we are going to use has the following form\n",
    "\n",
    "$$ E(\\mathbf{w}, \\lambda) = \\frac{1}{N}\\sum_{n=1}^N (y_n - f(\\mathbf{x}_n))^2 + \\frac{\\lambda}{2}\\sum_{j=0}^D w_j^2,$$\n",
    "\n",
    "where $\\lambda>0$ is known as the *regularisation* parameter.\n",
    "\n",
    "This objective function was studied in Lecture 4. \n",
    "\n",
    "The first term on the rhs is what we call the \"fitting\" term whereas the second term in the expression is the regularisation term. Given $\\lambda$, the two terms in the expression have different purposes. The first term is looking for a value of $\\mathbf{w}$ that leads the squared-errors to zero. While doing this, $\\mathbf{w}$ can take any value and lead to a solution that it is only good for the training data but perhaps not for the test data. The second term is regularising the behavior of the first term by driving the $\\mathbf{w}$ towards zero. By doing this, it restricts the possible set of values that $\\mathbf{w}$ might take according to the first term. The value that we use for $\\lambda$ will allow a compromise between a value of $\\mathbf{w}$ that exactly fits the data (first term) or a value of $\\mathbf{w}$ that does not grow too much (second term).\n",
    "\n",
    "This type of regularisation has different names: ridge regression, Tikhonov regularisation or $\\ell_2$ norm regularisation. \n",
    "\n",
    "## 1.2 Optimising the objective function with respect to $\\mathbf{w}$\n",
    "\n",
    "There are two ways we can optimise the objective function with respect to $\\mathbf{w}$. The first one leads to a closed form expression for $\\mathbf{w}$ and the second one using an iterative optimisation procedure that updates the value of $\\mathbf{w}$ at each iteration by using the gradient of the objective function with respect to $\\mathbf{w}$,\n",
    "$$\n",
    "\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d E(\\mathbf{w}, \\lambda)}{d\\mathbf{w}},\n",
    "$$\n",
    "where $\\eta$ is the *learning rate* parameter and $\\frac{d E(\\mathbf{w}, \\lambda)}{d\\mathbf{w}}$ is the gradient of the objective function.\n",
    "\n",
    "It can be shown (this is a question in the Exercise Sheet 4) that a closed-form expression for the optimal $\\mathbf{w}_*$ is given as\n",
    "\n",
    "\\begin{align*}            \n",
    "            \\mathbf{w}_*& = \\left(\\mathbf{X}^{\\top}\\mathbf{X} + \\frac{\\lambda N}   \n",
    "                                     {2}\\mathbf{I}\\right)^{-1}\\mathbf{X}^{\\top}\\mathbf{y}.\n",
    "\\end{align*}\n",
    "\n",
    "Alternatively, we can find an update equation for $\\mathbf{w}_{\\text{new}}$ using gradient descent leading to:\n",
    "\n",
    "\\begin{align*}\n",
    "   \\mathbf{w}_{\\text{new}} & = \\mathbf{w}_{\\text{old}} - \\eta \\frac{d E(\\mathbf{w}, \\lambda)}\n",
    "                              {d\\mathbf{w}},\\\\\n",
    "                           & = \\mathbf{w}_{\\text{old}} +  \\frac{2\\eta}{N}\\sum_{n=1}^N   \n",
    "                               \\left(y_n - \\mathbf{x}_n^{\\top}\\mathbf{w}_{\\text{old}}\\right)\\mathbf{x}_n  \n",
    "                       - \\eta\\lambda\\mathbf{w}_{\\text{old}}\\\\\n",
    "                           & = (1 - \\eta\\lambda)\\mathbf{w}_{\\text{old}} + \\frac{2\\eta}\n",
    "                               {N}\\sum_{n=1}^N   \n",
    "                               \\left(y_n - \\mathbf{x}_n^{\\top}\\mathbf{w}_{\\text{old}}\\right)\\mathbf{x}_n\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Preprocessing the data\n",
    "\n",
    "As mentioned before, the dataset has missing values tagged with a -200 value. Before doing any work with the training data, we want to make sure that we deal properly with the missing values. Furthermore, once we have dealt with the missing values, we want to standardise the training data. \n",
    "\n",
    "### Question 1.a: Missing values and standardisation (2 marks)\n",
    "\n",
    "* For all the other features with missing values, use the mean value of the non-missing values for that feature to perform imputation. Save these mean values, you will need them when performing the validation stage (**1 mark**).\n",
    "\n",
    "* Once you have imputed the missing data, we need to standardise the input vectors. Standardise the training data by substracting the mean value for each feature and dividing the result by the standard deviation of each feature. Keep the mean values and standard deviations, you will need them at validation time (**1 mark**).\n",
    "\n",
    "#### Question 1.a Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Mean Imputation for missing values\n",
    "def mean_imputation(data, means=None):\n",
    "    if(means is None):\n",
    "        means = pd.DataFrame()\n",
    "        data_temp = data.replace(-200, 0)\n",
    "        # Process each column with data_temp\n",
    "        for column in data_temp.columns:\n",
    "            mean = data_temp[column].mean()\n",
    "            data[column].replace(-200, mean, inplace=True)\n",
    "            means[column] = [mean]\n",
    "        return data, means\n",
    "    else:\n",
    "        data_temp = data.replace(-200, 0)\n",
    "        # Process each column with data_temp\n",
    "        for column in data_temp.columns:\n",
    "            data[column].replace(-200, means[column][0], inplace=True)\n",
    "        return data, _\n",
    "\n",
    "# Standardization\n",
    "def standardise(data, means=None, stds=None):\n",
    "    if((means is None) & (stds is None)):\n",
    "        means = pd.DataFrame()\n",
    "        stds = pd.DataFrame()\n",
    "        for column in data.columns:\n",
    "            #standardize the values in each column\n",
    "            mean = data[column].mean()\n",
    "            std = data[column].std()\n",
    "            data[column] = (data[column]-mean)/std\n",
    "            means[column] = [mean]\n",
    "            stds[column] = [std]\n",
    "        return data, means, stds\n",
    "    else:\n",
    "        for column in data.columns:\n",
    "            #standardize the values in each column\n",
    "            data[column] = (data[column]-means[column][0])/stds[column][0]\n",
    "        return data, _, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare the training data\n",
    "XTrain_unproc = data_training_unproc.iloc[:,1:]\n",
    "\n",
    "XTrain_inproc, column_means_unproc = mean_imputation(XTrain_unproc.copy())\n",
    "XTrain, column_means_imput, column_stds_imput = standardise(XTrain_inproc.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Training and validation stages\n",
    "\n",
    "We have now curated our training data by removing data observations and features with a large amount of missing values. We have also normalised the feature vectors. We are now in a good position to work on developing the prediction model and validating it. We will use gradient descent for iterative optimisation. \n",
    "\n",
    "We first organise the dataframe into the vector of targets $\\mathbf{y}$, call it `yTrain`, and the design matrix $\\mathbf{X}$, call it `XTrain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Get the training data\n",
    "XTrain = XTrain\n",
    "yTrain = data_training_unproc.iloc[:,0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.b: finding the optimal $\\mathbf{w}$ with stochastic gradient descent (3 marks)\n",
    "\n",
    "Use gradient descent to iteratively compute the value of $\\mathbf{w}_{\\text{new}}$. Instead of using all the training set in `XTrain` and `yTrain` to compute the gradient, use a subset of $S$ instances in `XTrain` and `yTrain`. This is sometimes called *minibatch gradient descent* where $S$ is the size of the minibacth. When using gradient descent with minibatches, you need to find the best values for three parameters: $\\eta$, the learning rate, $S$, the number of datapoints in the minibatch and $\\gamma$, the regularisation parameter.\n",
    "\n",
    "* In this question we will use the validation data. So before proceeding to the next steps, make sure that you:  replace the missing values on each feature variables with the mean value you computed with the training data; standardise the validation data using the means and standard deviations computed from the training data (**1 mark**).\n",
    "    \n",
    "* Create a grid of values for the parameters $\\gamma$ and $\\eta$ using `np.logspace` and a grid of values for $S$ using `np.linspace`. Because you need to find three parameters, start with five values for each parameter in the grid and see if you can increase it. Make sure you understand what is the meaning of `np.logspace` and `np.linspace`. Notice that you can use negative values for `start` in `np.logspace` (**1 mark**).\n",
    "\n",
    "* For each value that you have of $\\gamma$, $\\eta$ and $S$ from the previous step, use the training set to compute $\\mathbf{w}$ using minibatch gradient descent and then measure the RMSE over the validation data. For the validation data, make sure you preprocess it before applying the prediction model over it. For the minibatch gradient descent choose to stop the iterative procedure after $200$ iterations (**1 mark**).\n",
    "\n",
    "* Choose the values of $\\gamma$, $\\eta$ and $S$ that lead to the lowest RMSE and save them. You will use them at the test stage.\n",
    "\n",
    "#### Question 1.b Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the validation data\n",
    "XValid_unproc = data_val_unproc.iloc[:,1:]\n",
    "\n",
    "XValid_inproc, _ = mean_imputation(XValid_unproc.copy(), column_means_unproc)\n",
    "XValid, _, _ = standardise(XValid_inproc.copy(), column_means_imput, column_stds_imput)\n",
    "\n",
    "XValid = XValid\n",
    "yValid = data_val_unproc.iloc[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check training data\n",
    "# print(type(XTrain))\n",
    "# print(XTrain.head(3))\n",
    "# print(type(yTrain))\n",
    "# print(yTrain.head(3))\n",
    "\n",
    "# # check validation data\n",
    "# print(type(XValid))\n",
    "# print(XValid.head(3))\n",
    "# print(type(yValid))\n",
    "# print(yValid.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a list containing mini-batches\n",
    "def create_mini_batches(X, y, batch_size, shuffle=True):\n",
    "    # Config\n",
    "    mini_batches = []\n",
    "    data = np.hstack((X, y))\n",
    "    n_minibatches = data.shape[0] // batch_size\n",
    "    i = 0\n",
    "    \n",
    "    # Shuffle data\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)\n",
    "    \n",
    "    # Create mini-batches\n",
    "    for i in range(n_minibatches + 1):\n",
    "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]\n",
    "        X_mini = mini_batch[:, :-1]\n",
    "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "    if data.shape[0] % batch_size != 0:\n",
    "        mini_batch = data[i * batch_size:data.shape[0]]\n",
    "        X_mini = mini_batch[:, :-1]\n",
    "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "        \n",
    "    return mini_batches\n",
    "\n",
    "# # Test create_mini_batches function\n",
    "# print(type(XTrain))\n",
    "# Train_batches = create_mini_batches(XTrain, yTrain, 8)\n",
    "\n",
    "# # Check result\n",
    "# for batch in Train_batches:\n",
    "#     print(batch[0].shape)\n",
    "#     print(batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    \"\"\"Class to fit a ridge regression model on a given training set, and make predictions on data.\n",
    "\n",
    "    Specifically, the cost function with L_2 regularization is assumed to be:\n",
    "        ||w - Xw||^2 + l2_parameter * ||w||^2\n",
    "\n",
    "    where,\n",
    "        l2_parameter = Model weights,\n",
    "        X = Data design matrix,\n",
    "        alpha = regularization parameter.\n",
    "\n",
    "    The class uses batch gradient descent to optimize the model weights.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.0001, l2_parameter=0.2, max_iter=500, batch_size=16):\n",
    "        \"\"\"Initialize.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "            reg_strength (float): Regularization parameter, to control the bias-variance tradeoff.\n",
    "            max_iter (int): Number of iterations to run gradient descent.\n",
    "\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l2_parameter = l2_parameter\n",
    "        self.max_iter = max_iter\n",
    "        self.l2_parameter: np.array\n",
    "        self.batch_size = batch_size\n",
    "        self.error_list = []\n",
    "    \n",
    "    # linear regression using \"mini-batch\" gradient descent\n",
    "    # function to compute hypothesis / predictions\n",
    "    def function(self, X):\n",
    "        return np.dot(X, self.Weight)\n",
    "\n",
    "    # function to compute gradient of error function w.r.t. theta\n",
    "    def grad(self, X, y_true):\n",
    "        y_pred = self.function(X)\n",
    "        grad = 2*np.dot(X.transpose(), (y_pred-y_true))/X.shape[0] + self.l2_parameter*self.Weight\n",
    "        return grad\n",
    "\n",
    "    # function to compute the error for current values of theta\n",
    "    def loss(self, X, y_true):\n",
    "        y_pred = self.function(X)\n",
    "        loss = np.dot((y_pred-y_true).transpose(), (y_pred-y_true))/X.shape[0] + self.l2_parameter*np.dot(self.Weight.transpose(), self.Weight)/2\n",
    "        return loss[0]\n",
    "\n",
    "    def validate(self, XValid, yValid):\n",
    "        y_pred = self.function(XValid)\n",
    "        MSE = np.square(np.subtract(yValid, y_pred)).mean()\n",
    "        RMSE = math.sqrt(MSE)\n",
    "        return RMSE\n",
    "    \n",
    "    # function to perform mini-batch gradient descent\n",
    "    def fit(self, X, y, XValid, yValid):\n",
    "        self.Weight = np.zeros((X.shape[1], 1))\n",
    "        for itr in range(self.max_iter):\n",
    "            mini_batches = create_mini_batches(X, y, self.batch_size)\n",
    "            for mini_batch in mini_batches:\n",
    "                X_mini, y_mini = mini_batch\n",
    "                self.Weight = self.Weight - self.learning_rate * self.grad(X_mini, y_mini)\n",
    "                training_loss = self.loss(X_mini, y_mini)\n",
    "                self.error_list.append(training_loss)\n",
    "        validation_loss = self.validate(XValid, yValid)\n",
    "        print(\"training loss:\", training_loss[0], \"validation loss:\", validation_loss)\n",
    "        return training_loss[0], validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training example\n",
    "# model = RidgeRegression()\n",
    "# model.fit(XTrain, yTrain, XValid, yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "𝛾: [0.1        0.17782794 0.31622777 0.56234133 1.        ]\n",
      "𝜂: [1.e-05 1.e-04 1.e-03 1.e-02 1.e-01]\n",
      "𝑆: [ 8 16 24 32 40 48 56 64]\n"
     ]
    }
   ],
   "source": [
    "# 𝜂, the learning rate\n",
    "# 𝑆, the number of datapoints in the minibatch/batch size\n",
    "# 𝛾, the regularisation parameter\n",
    "𝛾 = np.logspace(-1, 0, num=5)\n",
    "print(\"𝛾:\", 𝛾)\n",
    "𝜂 = np.logspace(-5.0, -1.0, num=5)\n",
    "print(\"𝜂:\", 𝜂)\n",
    "𝑆 = np.linspace(8.0, 64.0, num=8, dtype=int)\n",
    "print(\"𝑆:\", 𝑆)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "𝛾: 0.1 𝜂: 1e-05 𝑆: 8  training loss: 4.655791787959439 validation loss: 2.2265877848356155\n",
      "𝛾: 0.1 𝜂: 1e-05 𝑆: 16  training loss: 5.614450940494148 validation loss: 2.228171852310261\n",
      "𝛾: 0.1 𝜂: 1e-05 𝑆: 24  training loss: 4.685083313311158 validation loss: 2.2295846524605376\n",
      "𝛾: 0.1 𝜂: 1e-05 𝑆: 32  training loss: 4.203546076563292 validation loss: 2.2308256457213447\n",
      "𝛾: 0.1 𝜂: 1e-05 𝑆: 40  training loss: 4.643870648074904 validation loss: 2.231727754350374\n",
      "𝛾: 0.1 𝜂: 1e-05 𝑆: 48  training loss: 4.734154119814896 validation loss: 2.232433287431211\n",
      "𝛾: 0.1 𝜂: 1e-05 𝑆: 56  training loss: 5.076106115285331 validation loss: 2.23293877387355\n",
      "𝛾: 0.1 𝜂: 1e-05 𝑆: 64  training loss: 5.613231254818008 validation loss: 2.233166018847822\n",
      "𝛾: 0.1 𝜂: 0.0001 𝑆: 8  training loss: 4.847266477308475 validation loss: 2.224518088300719\n",
      "𝛾: 0.1 𝜂: 0.0001 𝑆: 16  training loss: 4.479827608172199 validation loss: 2.2244294345969515\n",
      "𝛾: 0.1 𝜂: 0.0001 𝑆: 24  training loss: 6.227807205351317 validation loss: 2.2245924244690287\n",
      "𝛾: 0.1 𝜂: 0.0001 𝑆: 32  training loss: 5.524555701306592 validation loss: 2.224949142565636\n",
      "𝛾: 0.1 𝜂: 0.0001 𝑆: 40  training loss: 4.698773296940785 validation loss: 2.22539913512083\n",
      "𝛾: 0.1 𝜂: 0.0001 𝑆: 48  training loss: 5.4913769498410785 validation loss: 2.2255772721040707\n",
      "𝛾: 0.1 𝜂: 0.0001 𝑆: 56  training loss: 5.073084228208997 validation loss: 2.2258001873318376\n",
      "𝛾: 0.1 𝜂: 0.0001 𝑆: 64  training loss: 5.905550290007136 validation loss: 2.2260284116667712\n",
      "𝛾: 0.1 𝜂: 0.001 𝑆: 8  training loss: 6.518985864850193 validation loss: 2.2367345529334197\n",
      "𝛾: 0.1 𝜂: 0.001 𝑆: 16  training loss: 4.617832297803868 validation loss: 2.226458796271201\n",
      "𝛾: 0.1 𝜂: 0.001 𝑆: 24  training loss: 4.38669926572936 validation loss: 2.2251735725398576\n",
      "𝛾: 0.1 𝜂: 0.001 𝑆: 32  training loss: 5.386993629945919 validation loss: 2.223352381117241\n",
      "𝛾: 0.1 𝜂: 0.001 𝑆: 40  training loss: 4.93767317075698 validation loss: 2.225291627295192\n",
      "𝛾: 0.1 𝜂: 0.001 𝑆: 48  training loss: 5.332174746895643 validation loss: 2.2252453372511685\n",
      "𝛾: 0.1 𝜂: 0.001 𝑆: 56  training loss: 4.75269615297147 validation loss: 2.224245075704054\n",
      "𝛾: 0.1 𝜂: 0.001 𝑆: 64  training loss: 4.738102965067118 validation loss: 2.224892454127316\n",
      "𝛾: 0.1 𝜂: 0.01 𝑆: 8  training loss: 3.9445890460568735 validation loss: 2.2303544696155653\n",
      "𝛾: 0.1 𝜂: 0.01 𝑆: 16  training loss: 4.564386883093787 validation loss: 2.2617242984381933\n",
      "𝛾: 0.1 𝜂: 0.01 𝑆: 24  training loss: 4.581951545052884 validation loss: 2.2396959545153465\n",
      "𝛾: 0.1 𝜂: 0.01 𝑆: 32  training loss: 5.553023048798726 validation loss: 2.235482313248128\n",
      "𝛾: 0.1 𝜂: 0.01 𝑆: 40  training loss: 4.660623700689536 validation loss: 2.2253120044499584\n",
      "𝛾: 0.1 𝜂: 0.01 𝑆: 48  training loss: 5.353839422485999 validation loss: 2.2254708940392027\n",
      "𝛾: 0.1 𝜂: 0.01 𝑆: 56  training loss: 4.868178549747168 validation loss: 2.2431464516870907\n",
      "𝛾: 0.1 𝜂: 0.01 𝑆: 64  training loss: 4.701631490932277 validation loss: 2.220412427064694\n",
      "𝛾: 0.1 𝜂: 0.1 𝑆: 8  training loss: 4.503658935044389 validation loss: 2.908267677178938\n",
      "𝛾: 0.1 𝜂: 0.1 𝑆: 16  training loss: 3.2273820174377215 validation loss: 2.3874101792970666\n",
      "𝛾: 0.1 𝜂: 0.1 𝑆: 24  training loss: 3.4862917175344417 validation loss: 2.423623292397339\n",
      "𝛾: 0.1 𝜂: 0.1 𝑆: 32  training loss: 4.955044187043374 validation loss: 2.255603194788205\n",
      "𝛾: 0.1 𝜂: 0.1 𝑆: 40  training loss: 2.3753093640134653 validation loss: 2.5148463996618644\n",
      "𝛾: 0.1 𝜂: 0.1 𝑆: 48  training loss: 4.531828894015036 validation loss: 2.3006761935946924\n",
      "𝛾: 0.1 𝜂: 0.1 𝑆: 56  training loss: 4.823883559483988 validation loss: 2.226358156745797\n",
      "𝛾: 0.1 𝜂: 0.1 𝑆: 64  training loss: 4.952246032768508 validation loss: 2.23232612179272\n",
      "𝛾: 0.1778279410038923 𝜂: 1e-05 𝑆: 8  training loss: 5.28861707607875 validation loss: 2.226314478511682\n",
      "𝛾: 0.1778279410038923 𝜂: 1e-05 𝑆: 16  training loss: 4.464636096937584 validation loss: 2.227880799225327\n",
      "𝛾: 0.1778279410038923 𝜂: 1e-05 𝑆: 24  training loss: 4.796147162929669 validation loss: 2.2293523920999085\n",
      "𝛾: 0.1778279410038923 𝜂: 1e-05 𝑆: 32  training loss: 4.390478319493928 validation loss: 2.230513071145439\n",
      "𝛾: 0.1778279410038923 𝜂: 1e-05 𝑆: 40  training loss: 4.615594024146964 validation loss: 2.231398589432929\n",
      "𝛾: 0.1778279410038923 𝜂: 1e-05 𝑆: 48  training loss: 5.400304801911624 validation loss: 2.2320888997625556\n",
      "𝛾: 0.1778279410038923 𝜂: 1e-05 𝑆: 56  training loss: 5.924891454581863 validation loss: 2.2325288222755795\n",
      "𝛾: 0.1778279410038923 𝜂: 1e-05 𝑆: 64  training loss: 5.2852571227427 validation loss: 2.2328382620642233\n",
      "𝛾: 0.1778279410038923 𝜂: 0.0001 𝑆: 8  training loss: 5.560625164298805 validation loss: 2.225505399490684\n",
      "𝛾: 0.1778279410038923 𝜂: 0.0001 𝑆: 16  training loss: 5.622578229121236 validation loss: 2.224912481784592\n",
      "𝛾: 0.1778279410038923 𝜂: 0.0001 𝑆: 24  training loss: 4.7299996880937965 validation loss: 2.2247751691231707\n",
      "𝛾: 0.1778279410038923 𝜂: 0.0001 𝑆: 32  training loss: 5.260051165871815 validation loss: 2.2250622802925473\n",
      "𝛾: 0.1778279410038923 𝜂: 0.0001 𝑆: 40  training loss: 5.1421336419972645 validation loss: 2.2253425311929638\n",
      "𝛾: 0.1778279410038923 𝜂: 0.0001 𝑆: 48  training loss: 4.815895863064338 validation loss: 2.225453234799053\n",
      "𝛾: 0.1778279410038923 𝜂: 0.0001 𝑆: 56  training loss: 4.836768543124639 validation loss: 2.2257219311758103\n",
      "𝛾: 0.1778279410038923 𝜂: 0.0001 𝑆: 64  training loss: 4.67595062977986 validation loss: 2.2259285986101087\n",
      "𝛾: 0.1778279410038923 𝜂: 0.001 𝑆: 8  training loss: 4.494952840326138 validation loss: 2.2374358417897975\n",
      "𝛾: 0.1778279410038923 𝜂: 0.001 𝑆: 16  training loss: 4.366001211339792 validation loss: 2.2246646885562606\n",
      "𝛾: 0.1778279410038923 𝜂: 0.001 𝑆: 24  training loss: 4.432215373503951 validation loss: 2.2245655680865104\n",
      "𝛾: 0.1778279410038923 𝜂: 0.001 𝑆: 32  training loss: 4.381306493804596 validation loss: 2.2253502016081814\n",
      "𝛾: 0.1778279410038923 𝜂: 0.001 𝑆: 40  training loss: 5.0471723064207525 validation loss: 2.2250191656787197\n",
      "𝛾: 0.1778279410038923 𝜂: 0.001 𝑆: 48  training loss: 5.467098211407153 validation loss: 2.224310995570367\n",
      "𝛾: 0.1778279410038923 𝜂: 0.001 𝑆: 56  training loss: 4.888810669384891 validation loss: 2.2252952131361514\n",
      "𝛾: 0.1778279410038923 𝜂: 0.001 𝑆: 64  training loss: 5.0028138318867885 validation loss: 2.2255377635303986\n",
      "𝛾: 0.1778279410038923 𝜂: 0.01 𝑆: 8  training loss: 4.390400871264506 validation loss: 2.274441840787194\n",
      "𝛾: 0.1778279410038923 𝜂: 0.01 𝑆: 16  training loss: 5.105775060376174 validation loss: 2.2366045926745204\n",
      "𝛾: 0.1778279410038923 𝜂: 0.01 𝑆: 24  training loss: 4.407876681445129 validation loss: 2.231193213131015\n",
      "𝛾: 0.1778279410038923 𝜂: 0.01 𝑆: 32  training loss: 3.878210359203809 validation loss: 2.238606646600869\n",
      "𝛾: 0.1778279410038923 𝜂: 0.01 𝑆: 40  training loss: 4.513643781477511 validation loss: 2.222660227594806\n",
      "𝛾: 0.1778279410038923 𝜂: 0.01 𝑆: 48  training loss: 4.628156218604368 validation loss: 2.226184193732295\n",
      "𝛾: 0.1778279410038923 𝜂: 0.01 𝑆: 56  training loss: 4.650109555089216 validation loss: 2.2212835289293356\n",
      "𝛾: 0.1778279410038923 𝜂: 0.01 𝑆: 64  training loss: 5.138231907648468 validation loss: 2.232375630418313\n",
      "𝛾: 0.1778279410038923 𝜂: 0.1 𝑆: 8  training loss: 0.8463023596065669 validation loss: 2.528775357771512\n",
      "𝛾: 0.1778279410038923 𝜂: 0.1 𝑆: 16  training loss: 4.589319256290493 validation loss: 2.271298695883138\n",
      "𝛾: 0.1778279410038923 𝜂: 0.1 𝑆: 24  training loss: 3.9645438736168868 validation loss: 2.3451379360999027\n",
      "𝛾: 0.1778279410038923 𝜂: 0.1 𝑆: 32  training loss: 3.6638986824771154 validation loss: 2.3139851207662034\n",
      "𝛾: 0.1778279410038923 𝜂: 0.1 𝑆: 40  training loss: 4.550818992695977 validation loss: 2.3162074851189063\n",
      "𝛾: 0.1778279410038923 𝜂: 0.1 𝑆: 48  training loss: 4.829538680447448 validation loss: 2.2831984131535736\n",
      "𝛾: 0.1778279410038923 𝜂: 0.1 𝑆: 56  training loss: 4.024186042900951 validation loss: 2.3409159367736367\n",
      "𝛾: 0.1778279410038923 𝜂: 0.1 𝑆: 64  training loss: 4.574275027722566 validation loss: 2.2283984278574516\n",
      "𝛾: 0.31622776601683794 𝜂: 1e-05 𝑆: 8  training loss: 3.216263247001683 validation loss: 2.22618297627367\n",
      "𝛾: 0.31622776601683794 𝜂: 1e-05 𝑆: 16  training loss: 4.649930702389121 validation loss: 2.227606556722501\n",
      "𝛾: 0.31622776601683794 𝜂: 1e-05 𝑆: 24  training loss: 5.087443810577613 validation loss: 2.2288796873615095\n",
      "𝛾: 0.31622776601683794 𝜂: 1e-05 𝑆: 32  training loss: 4.484236270446922 validation loss: 2.230006664375871\n",
      "𝛾: 0.31622776601683794 𝜂: 1e-05 𝑆: 40  training loss: 4.207789956140256 validation loss: 2.2308631617779855\n",
      "𝛾: 0.31622776601683794 𝜂: 1e-05 𝑆: 48  training loss: 5.551552018474378 validation loss: 2.2315120377432045\n",
      "𝛾: 0.31622776601683794 𝜂: 1e-05 𝑆: 56  training loss: 5.413225065542568 validation loss: 2.231998024169885\n",
      "𝛾: 0.31622776601683794 𝜂: 1e-05 𝑆: 64  training loss: 4.748155792882414 validation loss: 2.2322977721379575\n",
      "𝛾: 0.31622776601683794 𝜂: 0.0001 𝑆: 8  training loss: 3.5971916604790937 validation loss: 2.2250382738933236\n",
      "𝛾: 0.31622776601683794 𝜂: 0.0001 𝑆: 16  training loss: 5.103996679979289 validation loss: 2.2254389516095\n",
      "𝛾: 0.31622776601683794 𝜂: 0.0001 𝑆: 24  training loss: 4.652303734226177 validation loss: 2.2254842779470763\n",
      "𝛾: 0.31622776601683794 𝜂: 0.0001 𝑆: 32  training loss: 5.645794899808987 validation loss: 2.2255143228846115\n",
      "𝛾: 0.31622776601683794 𝜂: 0.0001 𝑆: 40  training loss: 4.170488616075391 validation loss: 2.22543134417914\n",
      "𝛾: 0.31622776601683794 𝜂: 0.0001 𝑆: 48  training loss: 5.359836696974932 validation loss: 2.2255483453699414\n",
      "𝛾: 0.31622776601683794 𝜂: 0.0001 𝑆: 56  training loss: 5.41518720490723 validation loss: 2.225770886499063\n",
      "𝛾: 0.31622776601683794 𝜂: 0.0001 𝑆: 64  training loss: 4.78903234072645 validation loss: 2.2258592415325436\n",
      "𝛾: 0.31622776601683794 𝜂: 0.001 𝑆: 8  training loss: 3.7958010856667386 validation loss: 2.2273870240849307\n",
      "𝛾: 0.31622776601683794 𝜂: 0.001 𝑆: 16  training loss: 6.735956933465386 validation loss: 2.2264896646324766\n",
      "𝛾: 0.31622776601683794 𝜂: 0.001 𝑆: 24  training loss: 5.180761991000077 validation loss: 2.226481409787215\n",
      "𝛾: 0.31622776601683794 𝜂: 0.001 𝑆: 32  training loss: 4.627451604152825 validation loss: 2.2257041659013903\n",
      "𝛾: 0.31622776601683794 𝜂: 0.001 𝑆: 40  training loss: 4.807019310163129 validation loss: 2.2250220700022383\n",
      "𝛾: 0.31622776601683794 𝜂: 0.001 𝑆: 48  training loss: 4.713162079786707 validation loss: 2.225589605347353\n",
      "𝛾: 0.31622776601683794 𝜂: 0.001 𝑆: 56  training loss: 5.28255850841249 validation loss: 2.2257571599989734\n",
      "𝛾: 0.31622776601683794 𝜂: 0.001 𝑆: 64  training loss: 5.089012553992836 validation loss: 2.225525749287896\n",
      "𝛾: 0.31622776601683794 𝜂: 0.01 𝑆: 8  training loss: 5.318258896006464 validation loss: 2.2462433981667393\n",
      "𝛾: 0.31622776601683794 𝜂: 0.01 𝑆: 16  training loss: 6.312024136268206 validation loss: 2.2279837781076917\n",
      "𝛾: 0.31622776601683794 𝜂: 0.01 𝑆: 24  training loss: 4.90522733741622 validation loss: 2.22285703572425\n",
      "𝛾: 0.31622776601683794 𝜂: 0.01 𝑆: 32  training loss: 4.802649641106518 validation loss: 2.2240290032703185\n",
      "𝛾: 0.31622776601683794 𝜂: 0.01 𝑆: 40  training loss: 5.622801240358436 validation loss: 2.2219576961112972\n",
      "𝛾: 0.31622776601683794 𝜂: 0.01 𝑆: 48  training loss: 4.441322825062595 validation loss: 2.2254782377800084\n",
      "𝛾: 0.31622776601683794 𝜂: 0.01 𝑆: 56  training loss: 4.980517002582863 validation loss: 2.226684190799901\n",
      "𝛾: 0.31622776601683794 𝜂: 0.01 𝑆: 64  training loss: 4.259745057873763 validation loss: 2.221908919150108\n",
      "𝛾: 0.31622776601683794 𝜂: 0.1 𝑆: 8  training loss: 15.231531582385127 validation loss: 3.5490541456461475\n",
      "𝛾: 0.31622776601683794 𝜂: 0.1 𝑆: 16  training loss: 5.280375225304762 validation loss: 2.3907512806978457\n",
      "𝛾: 0.31622776601683794 𝜂: 0.1 𝑆: 24  training loss: 4.0955575182639725 validation loss: 2.2373906832927926\n",
      "𝛾: 0.31622776601683794 𝜂: 0.1 𝑆: 32  training loss: 4.748110780540292 validation loss: 2.2802158462626707\n",
      "𝛾: 0.31622776601683794 𝜂: 0.1 𝑆: 40  training loss: 2.765098743576325 validation loss: 2.256912071533718\n",
      "𝛾: 0.31622776601683794 𝜂: 0.1 𝑆: 48  training loss: 5.199992397592853 validation loss: 2.240843628437317\n",
      "𝛾: 0.31622776601683794 𝜂: 0.1 𝑆: 56  training loss: 4.769812488859958 validation loss: 2.239324385305725\n",
      "𝛾: 0.31622776601683794 𝜂: 0.1 𝑆: 64  training loss: 4.3770726449887185 validation loss: 2.281822331484203\n",
      "𝛾: 0.5623413251903491 𝜂: 1e-05 𝑆: 8  training loss: 4.366499120073188 validation loss: 2.226304327228542\n",
      "𝛾: 0.5623413251903491 𝜂: 1e-05 𝑆: 16  training loss: 4.3323382747629156 validation loss: 2.227376175558272\n",
      "𝛾: 0.5623413251903491 𝜂: 1e-05 𝑆: 24  training loss: 4.412217884216482 validation loss: 2.2284353368506995\n",
      "𝛾: 0.5623413251903491 𝜂: 1e-05 𝑆: 32  training loss: 4.493264167307176 validation loss: 2.229390593851548\n",
      "𝛾: 0.5623413251903491 𝜂: 1e-05 𝑆: 40  training loss: 5.207199359529678 validation loss: 2.2301515658046975\n",
      "𝛾: 0.5623413251903491 𝜂: 1e-05 𝑆: 48  training loss: 5.082315840222579 validation loss: 2.230786829277532\n",
      "𝛾: 0.5623413251903491 𝜂: 1e-05 𝑆: 56  training loss: 4.713413619487773 validation loss: 2.2312364931083573\n",
      "𝛾: 0.5623413251903491 𝜂: 1e-05 𝑆: 64  training loss: 5.19722450994705 validation loss: 2.2316065852747298\n",
      "𝛾: 0.5623413251903491 𝜂: 0.0001 𝑆: 8  training loss: 6.304691980266105 validation loss: 2.2260591724935557\n",
      "𝛾: 0.5623413251903491 𝜂: 0.0001 𝑆: 16  training loss: 5.450751027482099 validation loss: 2.2260994627528254\n",
      "𝛾: 0.5623413251903491 𝜂: 0.0001 𝑆: 24  training loss: 4.279457859407527 validation loss: 2.2259708385928487\n",
      "𝛾: 0.5623413251903491 𝜂: 0.0001 𝑆: 32  training loss: 4.830883907303214 validation loss: 2.225944445078314\n",
      "𝛾: 0.5623413251903491 𝜂: 0.0001 𝑆: 40  training loss: 5.4766546170155905 validation loss: 2.2261143024183654\n",
      "𝛾: 0.5623413251903491 𝜂: 0.0001 𝑆: 48  training loss: 4.9825016048114295 validation loss: 2.226044248407932\n",
      "𝛾: 0.5623413251903491 𝜂: 0.0001 𝑆: 56  training loss: 4.686447404627516 validation loss: 2.2261587116195525\n",
      "𝛾: 0.5623413251903491 𝜂: 0.0001 𝑆: 64  training loss: 4.653761981991456 validation loss: 2.2261400115454704\n",
      "𝛾: 0.5623413251903491 𝜂: 0.001 𝑆: 8  training loss: 5.41446428116677 validation loss: 2.230158310969076\n",
      "𝛾: 0.5623413251903491 𝜂: 0.001 𝑆: 16  training loss: 5.443379962535932 validation loss: 2.226046834399553\n",
      "𝛾: 0.5623413251903491 𝜂: 0.001 𝑆: 24  training loss: 5.03303520962255 validation loss: 2.226017600898569\n",
      "𝛾: 0.5623413251903491 𝜂: 0.001 𝑆: 32  training loss: 4.673183002745088 validation loss: 2.2263695710983815\n",
      "𝛾: 0.5623413251903491 𝜂: 0.001 𝑆: 40  training loss: 5.597974852275241 validation loss: 2.225908842287685\n",
      "𝛾: 0.5623413251903491 𝜂: 0.001 𝑆: 48  training loss: 5.056631452654758 validation loss: 2.225758333437542\n",
      "𝛾: 0.5623413251903491 𝜂: 0.001 𝑆: 56  training loss: 5.628516054974179 validation loss: 2.2262551442041385\n",
      "𝛾: 0.5623413251903491 𝜂: 0.001 𝑆: 64  training loss: 4.889444630479116 validation loss: 2.225772438609344\n",
      "𝛾: 0.5623413251903491 𝜂: 0.01 𝑆: 8  training loss: 2.7936571926980744 validation loss: 2.271892776498844\n",
      "𝛾: 0.5623413251903491 𝜂: 0.01 𝑆: 16  training loss: 5.3655050380989175 validation loss: 2.229031953347425\n",
      "𝛾: 0.5623413251903491 𝜂: 0.01 𝑆: 24  training loss: 4.268301892803514 validation loss: 2.2297534747153556\n",
      "𝛾: 0.5623413251903491 𝜂: 0.01 𝑆: 32  training loss: 4.4668763371210805 validation loss: 2.243581489918916\n",
      "𝛾: 0.5623413251903491 𝜂: 0.01 𝑆: 40  training loss: 4.342450318152014 validation loss: 2.2600884186843273\n",
      "𝛾: 0.5623413251903491 𝜂: 0.01 𝑆: 48  training loss: 6.156611678516769 validation loss: 2.233250648562142\n",
      "𝛾: 0.5623413251903491 𝜂: 0.01 𝑆: 56  training loss: 4.669165465870095 validation loss: 2.2314301143258954\n",
      "𝛾: 0.5623413251903491 𝜂: 0.01 𝑆: 64  training loss: 5.338475426201324 validation loss: 2.2278785048569594\n",
      "𝛾: 0.5623413251903491 𝜂: 0.1 𝑆: 8  training loss: 0.9527471527554754 validation loss: 2.796096365816742\n",
      "𝛾: 0.5623413251903491 𝜂: 0.1 𝑆: 16  training loss: 5.217525139954329 validation loss: 2.308886898027072\n",
      "𝛾: 0.5623413251903491 𝜂: 0.1 𝑆: 24  training loss: 3.792880981683544 validation loss: 2.2915569594626226\n",
      "𝛾: 0.5623413251903491 𝜂: 0.1 𝑆: 32  training loss: 4.555860982942597 validation loss: 2.3148214121745228\n",
      "𝛾: 0.5623413251903491 𝜂: 0.1 𝑆: 40  training loss: 2.6558990083541136 validation loss: 2.4280440610133907\n",
      "𝛾: 0.5623413251903491 𝜂: 0.1 𝑆: 48  training loss: 4.651228905470755 validation loss: 2.233695026849831\n",
      "𝛾: 0.5623413251903491 𝜂: 0.1 𝑆: 56  training loss: 2.8950951020986695 validation loss: 2.4803774255748183\n",
      "𝛾: 0.5623413251903491 𝜂: 0.1 𝑆: 64  training loss: 5.007598227432707 validation loss: 2.22667059015535\n",
      "𝛾: 1.0 𝜂: 1e-05 𝑆: 8  training loss: 6.280427936651378 validation loss: 2.2270998421448414\n",
      "𝛾: 1.0 𝜂: 1e-05 𝑆: 16  training loss: 4.5996291197846695 validation loss: 2.2276283876877763\n",
      "𝛾: 1.0 𝜂: 1e-05 𝑆: 24  training loss: 5.156685359130303 validation loss: 2.228356307456813\n",
      "𝛾: 1.0 𝜂: 1e-05 𝑆: 32  training loss: 5.712899285813119 validation loss: 2.2290649223702053\n",
      "𝛾: 1.0 𝜂: 1e-05 𝑆: 40  training loss: 7.14762718417573 validation loss: 2.229714708355831\n",
      "𝛾: 1.0 𝜂: 1e-05 𝑆: 48  training loss: 5.5517618952269165 validation loss: 2.2302299010839315\n",
      "𝛾: 1.0 𝜂: 1e-05 𝑆: 56  training loss: 5.408990726563137 validation loss: 2.230655088174697\n",
      "𝛾: 1.0 𝜂: 1e-05 𝑆: 64  training loss: 5.357112024578288 validation loss: 2.231019678519241\n",
      "𝛾: 1.0 𝜂: 0.0001 𝑆: 8  training loss: 5.613243013684971 validation loss: 2.2271702845870873\n",
      "𝛾: 1.0 𝜂: 0.0001 𝑆: 16  training loss: 4.073805001366683 validation loss: 2.2271573440457457\n",
      "𝛾: 1.0 𝜂: 0.0001 𝑆: 24  training loss: 4.852735492385105 validation loss: 2.22711415478942\n",
      "𝛾: 1.0 𝜂: 0.0001 𝑆: 32  training loss: 5.993513674650585 validation loss: 2.2270987911213203\n",
      "𝛾: 1.0 𝜂: 0.0001 𝑆: 40  training loss: 5.03342422970054 validation loss: 2.2270810594896093\n",
      "𝛾: 1.0 𝜂: 0.0001 𝑆: 48  training loss: 5.275326631747865 validation loss: 2.2270921614338564\n",
      "𝛾: 1.0 𝜂: 0.0001 𝑆: 56  training loss: 5.284528930254073 validation loss: 2.2271452880030136\n",
      "𝛾: 1.0 𝜂: 0.0001 𝑆: 64  training loss: 5.174184415685316 validation loss: 2.2271125732072004\n",
      "𝛾: 1.0 𝜂: 0.001 𝑆: 8  training loss: 4.486760904837596 validation loss: 2.2281814144858223\n",
      "𝛾: 1.0 𝜂: 0.001 𝑆: 16  training loss: 5.521634169036042 validation loss: 2.2267594851301222\n",
      "𝛾: 1.0 𝜂: 0.001 𝑆: 24  training loss: 5.490808343934632 validation loss: 2.228390418897801\n",
      "𝛾: 1.0 𝜂: 0.001 𝑆: 32  training loss: 5.86956618370117 validation loss: 2.2266335766181977\n",
      "𝛾: 1.0 𝜂: 0.001 𝑆: 40  training loss: 4.66471126202778 validation loss: 2.2269680291019687\n",
      "𝛾: 1.0 𝜂: 0.001 𝑆: 48  training loss: 5.468573124357599 validation loss: 2.2273769028625554\n",
      "𝛾: 1.0 𝜂: 0.001 𝑆: 56  training loss: 4.740915457114991 validation loss: 2.2271987907891555\n",
      "𝛾: 1.0 𝜂: 0.001 𝑆: 64  training loss: 5.781162221769215 validation loss: 2.2271941939534954\n",
      "𝛾: 1.0 𝜂: 0.01 𝑆: 8  training loss: 4.197746471845665 validation loss: 2.2678979806748627\n",
      "𝛾: 1.0 𝜂: 0.01 𝑆: 16  training loss: 7.60414331717388 validation loss: 2.2660867672606613\n",
      "𝛾: 1.0 𝜂: 0.01 𝑆: 24  training loss: 4.57256796158056 validation loss: 2.230651076830074\n",
      "𝛾: 1.0 𝜂: 0.01 𝑆: 32  training loss: 5.264519654395723 validation loss: 2.2360332739983906\n",
      "𝛾: 1.0 𝜂: 0.01 𝑆: 40  training loss: 4.83628230464642 validation loss: 2.2438631255031134\n",
      "𝛾: 1.0 𝜂: 0.01 𝑆: 48  training loss: 5.740780191657818 validation loss: 2.2269320743623258\n",
      "𝛾: 1.0 𝜂: 0.01 𝑆: 56  training loss: 5.4581681609283415 validation loss: 2.22855918220658\n",
      "𝛾: 1.0 𝜂: 0.01 𝑆: 64  training loss: 4.917553435606999 validation loss: 2.226510461207953\n",
      "𝛾: 1.0 𝜂: 0.1 𝑆: 8  training loss: 2.1071097472822786 validation loss: 3.506740626223053\n",
      "𝛾: 1.0 𝜂: 0.1 𝑆: 16  training loss: 14.168877323719276 validation loss: 2.889441475355046\n",
      "𝛾: 1.0 𝜂: 0.1 𝑆: 24  training loss: 4.5214353802576674 validation loss: 2.3509229361705124\n",
      "𝛾: 1.0 𝜂: 0.1 𝑆: 32  training loss: 3.996617059643607 validation loss: 2.311550020992732\n",
      "𝛾: 1.0 𝜂: 0.1 𝑆: 40  training loss: 5.354789696068007 validation loss: 2.359713036302544\n",
      "𝛾: 1.0 𝜂: 0.1 𝑆: 48  training loss: 4.585896955561597 validation loss: 2.253744103845471\n",
      "𝛾: 1.0 𝜂: 0.1 𝑆: 56  training loss: 4.9238609866494505 validation loss: 2.23039740728499\n",
      "𝛾: 1.0 𝜂: 0.1 𝑆: 64  training loss: 5.483419097077783 validation loss: 2.2290024232791175\n",
      "best_score: 2.220412427064694 𝛾: 0.1 𝜂: 0.01 𝑆: 64\n"
     ]
    }
   ],
   "source": [
    "best_score = 100\n",
    "best_𝛾 = 100\n",
    "best_𝜂 = 100\n",
    "best_𝑆 = 100\n",
    "\n",
    "for i in 𝛾:\n",
    "    for j in 𝜂:\n",
    "        for k in 𝑆:\n",
    "            print(\"𝛾:\", i, \"𝜂:\", j, \"𝑆:\", k, end=\"  \")\n",
    "            model = RidgeRegression(learning_rate=j, l2_parameter=i, batch_size=k)\n",
    "            _, validation_loss = model.fit(XTrain, yTrain, XValid, yValid)\n",
    "            if (validation_loss!=None) & (validation_loss<best_score):\n",
    "                best_score = validation_loss\n",
    "                best_𝛾 = i\n",
    "                best_𝜂 = j\n",
    "                best_𝑆 = k\n",
    "print(\"best_score:\", best_score, \"𝛾:\", best_𝛾, \"𝜂:\", best_𝜂, \"𝑆:\", best_𝑆)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Test stage \n",
    "\n",
    "We now know which one is the best model, according to the validation data. We will now put together the training data and the validation data, perform the preprocessing as we did before, this is, treat the missing values and standardise the inputs. We will train the model again using the minibatch stochastic gradient descent and finally compute the RMSE over the test data.\n",
    "\n",
    "\n",
    "### Question 1.c: combine the original training and original validation data and perform the preprocessing again to this new data (2 marks)\n",
    "\n",
    "Put together the original training and validation dataset and perform the same preprocessing steps than before, these are: \n",
    "\n",
    "* for each feature, impute the missing values with the mean values of the non-missing values (**1 mark**) \n",
    "\n",
    "* stardardise the new training set (**1 mark**).\n",
    "\n",
    "#### Question 1.c Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "data_combination_unproc = air_quality.iloc[index[0:N+Nval], :].copy() # Select the combination data\n",
    "\n",
    "# Prepare the combination data\n",
    "XComb_unproc = data_combination_unproc.iloc[:,1:]\n",
    "\n",
    "XComb_inproc, column_means_unproc = mean_imputation(XComb_unproc.copy())\n",
    "XComb, column_means_imput, column_stds_imput = standardise(XComb_inproc.copy())\n",
    "yComb = data_combination_unproc.iloc[:, 0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.d: Preprocess the test data, train the model and predict over the test data (3 marks)\n",
    "\n",
    "Preprocess the test data and train a new model using the new training set. Finally, report the RMSE over the test set:\n",
    "\n",
    "* Preprocess the test data by imputing the missing data and standardising it (**1 mark**). \n",
    "\n",
    "* Use the best values of $\\gamma$, $\\eta$ and $S$ found in the validation set and train a new regularised linear model with stochastic gradient descent (**1 mark**).\n",
    "\n",
    "* Report the RMSE over the test data (**1 mark**).\n",
    "\n",
    "#### Question 1.d Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 4.63087477269101 validation loss: 2.212734755967431\n",
      "RMSE: 2.212734755967431\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "# Prepare the test data\n",
    "XTest_unproc = data_test_unproc.iloc[:,1:]\n",
    "XTest_inproc, _ = mean_imputation(XTest_unproc.copy(), column_means_unproc)\n",
    "XTest, _, _ = standardise(XTest_inproc.copy(), column_means_imput, column_stds_imput)\n",
    "yTest = data_test_unproc.iloc[:, 0:1]\n",
    "\n",
    "# Training with best values\n",
    "model = RidgeRegression(learning_rate=best_𝜂, l2_parameter=best_𝛾, batch_size=best_𝑆)\n",
    "_, RMSE = model.fit(XComb, yComb, XTest, yTest)\n",
    "print(\"RMSE:\", RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Random forests (13 marks)\n",
    "\n",
    "**USE scikit-learn for the questions on this section.**\n",
    "\n",
    "In section 1, you used a regularised ridge regression model trained with SGD to create a linear predictive model. In this part of the assignment, you will use **scikit-learn** to train a random forest for regression over the air quality dataset.\n",
    "\n",
    "## 2.1 Preprocessing the data\n",
    "\n",
    "As mentioned before, the dataset has missing values tagged with a -200 value. Before doing any work with the training data, we want to make sure that we deal properly with the missing values. Furthermore, once we have dealt with the missing values, we want to standardise the training data. \n",
    "\n",
    "### Question 2.a: Pipeline for missing values and standardisation (3 marks)\n",
    "\n",
    "* Employ the `SimpleImputer` method in Scikit-learn to impute the missing values in each column using the mean value of the non-missing values, instead (**1 mark**).\n",
    "\n",
    "* Standardise the data by substracting the mean value for each feature and dividing the result by the standard deviation of each feature. Employ the `StandardScaler` method (**1 mark**).\n",
    "\n",
    "* Create a `Pipeline` that you can use to preprocess the data by filling missing values and then standardising the features (**1 mark**).\n",
    "\n",
    "#### Question 2.a Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data_training_unproc = air_quality.iloc[index[0:N], :].copy() # Select the training data\n",
    "data_val_unproc = air_quality.iloc[index[N:N+Nval], :].copy() # Select the validation data\n",
    "data_test_unproc = air_quality.iloc[index[N+Nval:ndata], :].copy() # Select the test data\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=-200, strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "pipeline = Pipeline([('mean', imp_mean), ('scaler', scaler)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.b: Use the Pipeline to fit the training data and transform the validation data (2 marks)\n",
    "\n",
    "In the previous question, you created a `Pipeline` for applying a `SimpleImputer` and a `StandardScaler`. Use the Pipeline to fit the training data (**1 mark**) and transform the validation data (**1 mark**).\n",
    "\n",
    "#### Question 2.b Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "data_training_proc = pipeline.fit_transform(data_training_unproc)\n",
    "data_val_proc = pipeline.transform(data_val_unproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Random forest to predict air quality \n",
    "\n",
    "We now use random forests to predict air quality. Remember that the tree ensemble in random forests is built by training individual regression trees on different subsets of the training data and using a subset of the available features. For regression, the prediction is the average of the individual predictions of each tree. Some of the parameters required in the Random Forest implementation in Scikit-learn include:\n",
    "\n",
    "Some of the additional parameters required in the Random Forest implementation in Scikit-learn include\n",
    "\n",
    "> **n_estimators** the total number of trees to train<p>\n",
    "**max_features** number of features to use as candidates for splitting at each tree node. <p>\n",
    "    **boostrap**: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.<p>\n",
    "   **max_samples**: If bootstrap is True, the number of samples to draw from X to train each base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.c: train a random forest (4 marks)\n",
    "\n",
    "In this question, you will train a random forest for predicting over the validation data. Use cross-validation over the validation data to select the best set of paramaters for the random forest regressor. Parameters to include in your exploration are **n_estimators**, **max_features** and **max_samples**. Use `np.linspace` or `np.logspace` to define ranges of values to explore for each parameter and create a grid to be assessed over the validation data. Make sure you use the same validation data that was given to you.\n",
    "\n",
    "* Use `PredefinedSplit` to tell the cross validator which instances to use for training and which ones for validation (**1 mark**).\n",
    "\n",
    "* Create a grid of values to explore that include a range of at least five values for each parameter **n_estimators**, **max_features** and **max_samples** (**1 mark**). \n",
    "\n",
    "* Train a random forest for regression model that uses the grid of parameters you created before. Use `GridSearchCV` to find the best set of parameters by performing cross-validation over the predefined split. (**1 mark**).\n",
    "\n",
    "* Print the best values in your grid for **n_estimators**, **max_features** and **max_samples** (**1 mark**).\n",
    "\n",
    "#### Question 2.c Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "test_fold = np.zeros((np.shape(bs_train_set)[0], 1))\n",
    "test_fold[0:np.shape(bs_train2_set)[0]] = -1\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "# Parameters\n",
    "n_estimators_opts = [50, 75, 100, 125, 150]\n",
    "max_features_opts = [\"auto\", \"sqrt\", \"log2\", None, 20]\n",
    "boostrap_opts = [True, False]\n",
    "max_samples_opts = [None, 5, 10, 15, 0.5]\n",
    "param_grid = dict(n_estimators = n_estimators_opts, max_features = max_features_opts, boostrap = boostrap_opts, max_samples = max_samples_opts)\n",
    "\n",
    "grid_regression = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv=ps, scoring='neg_mean_squared_error')\n",
    "grid_regression.fit(whole_train_set_attributes, whole_train_set_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.d: train a new model over the whole training data and report the prediction over the test set (4 marks)\n",
    "\n",
    "\n",
    "Now that we have identified the best paramaters of the regression model, we use these parameters to train a new model over the whole training data (`data_training` plus `data_val`). We apply this model to the test set and report the performance.\n",
    "\n",
    "* Create a new preprocessing pipeline for taking care of the missing values and standardisation over the whole training data (**1 mark**).\n",
    "\n",
    "* Apply the created preprocessing pipeline to the test data (**1 mark**).\n",
    "\n",
    "* Fit a random forest regression model to the training data using the best parameters found in Question 2.c (**1 mark**).\n",
    "\n",
    "* Compute the RMSE over the test data and report the result (**1 mark**).\n",
    "\n",
    "#### Question 2.d Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
